#' get_predictions
#'
#' This is a function to get the predictions for a specific output simulated by SCOPE and calculate the accuracy.
#' @param SCOPE_dir the diretory patch of SCOPE
#' @param output_file default "fluxes.csv" for the energy fluxes output
#' @param pred_vec desirble output variables.
#' @param Simulation_Name simulation name stating the directory name generated by the run_SCOPE function.
#' @param obs_vec for get_accuracy a vector of observed values in the same timestamp as the predictions.
#' @param predictions the name of the file result of the get_predictions function.
#' @param metric_function for get_accuracy is possible to select the metric,
#' @return The result is a data.frame with all the predicted output per simulation for get_predictions, and
#'         a table with the metrics per simulation for get_accuracy.
#' @examples
#' Examples of uses of the get_predictions function
#' ###############
#' library(tidyverse)
#'
#' Predictions_pixel_1169 <- get_predictions(SCOPE_dir = "D:/SCOPE-master/",
#'                                           output_file = "fluxes.csv",
#'                                           pred_vec = "lEtot",
#'                                           Simulation_Name = "pixel_1169")
#'
#' Predictions_pixel_1169
#'
#' Examples of uses of the get_accuracy function
#'
#' possible metrics: yardstick::rmse for Root mean squared error,
#'                   yardstick::mae for Mean absolute error,
#'                   yardstick::msd for Mean signed deviation,
#'                   yardstick::ccc for Concordance correlation coefficient,
#'                   yardstick::mase for Mean absolute scaled error - order by time
#'                   yardstick::rsq for R squared - correlation
#'                   yardstick::rsq_trad for R squared - traditional
#'                   yardstick::rpiq for Ratio of performance to inter-quartile
#'                   yardstick::rpd for Ratio of performance to deviation
#'                   yardstick::iic for Index of ideality of correlation
#'                   yardstick::mpe for Mean percentage error
#'                   yardstick::mape for Mean absolute percent error
#'                   yardstick::smape for Symmetric mean absolute percentage error
#'
#' Get model accuracy for ET corrected
#' Predictions_metrics_1169 <- get_accuracy(obs_vec = EC_ROTH$ET_clean0,
#'                                          predictions = Predictions_pixel_1169,
#'                                          metric_function = yardstick::metric_set(yardstick::rsq,
#'                                                                                  yardstick::rmse,
#'                                                                                  yardstick::mae))
#' Predictions_metrics_1169
#'
#' @export
get_predictions <- function(
  SCOPE_dir = "D:/SCOPE-master/",
  Simulation_Name,
  output_file = "fluxes.csv",
  pred_vec = "lEtot"
){
  Outputs_files <- list.files(paste0(path=grep(Simulation_Name,
                                               list.dirs(path=paste0(SCOPE_dir,"output"),
                                                         full.names = TRUE,
                                                         recursive = FALSE),
                                               value = TRUE),
                                     "/", collapse = NULL, recycle0 = FALSE),
                              pattern= output_file,
                              full.names = TRUE)

  Outputs_list <- lapply(1:length(Outputs_files), function(i)  readr::read_csv(Outputs_files[i],
                                                                               col_names = T,
                                                                               readr::cols(.default = readr::col_double()))[-1,])

  for (i in 1:length(Outputs_list)) {
    colnames(Outputs_list[[i]])[which(colnames(Outputs_list[[i]]) == pred_vec)] <- "pred"
  }

  prediction <- data.frame(sapply(1:length(Outputs_files), function(i) Outputs_list[[i]]$pred))

  colnames(prediction) <- sapply(1:length(Outputs_list), function(i) stringr::str_split(Outputs_files, "/")[[i]][4])

  prediction <- tibble::tibble(prediction)

  return(prediction)

}

#' @describeIn get_predictions function
get_accuracy <- function(
  obs_vec,
  predictions,
  metric_function
){
  predictions_df <- predictions
  names(predictions_df) <- rep("pred",length(predictions_df))

  Predictions_metrics <- lapply(1:length(predictions_df), function(i) metric_function(
    predictions_df[i],
    truth = obs_vec,
    estimate = "pred", na_rm = TRUE))

  metrics_table <- data.frame(t(sapply(1:length(predictions_df), function(i) Predictions_metrics[[i]]$.estimate)))
  colnames(metrics_table) <- Predictions_metrics[[1]]$.metric
  rownames(metrics_table) <- names(predictions)

  metrics_table$rBias <- sapply(1:length(predictions), function(i) sum(predictions[i]-obs_vec, na.rm = T)/
                                  sum(obs_vec, na.rm = T))

  return(metrics_table)

}
